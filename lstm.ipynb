{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5b2497b3-60ee-7cd0-0625-f103214c0ed4"
   },
   "source": [
    "## Emotion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "6c53202d-5c34-4859-e7e9-8ef5c7068287",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishikesh/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics, cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data imported from: https://inclass.kaggle.com/c/sa-emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "89c8c923-c0bf-7b35-9ab8-e63f00b74e5a",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../text_emotion.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keeping the necessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>enthusiasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content   sentiment\n",
       "0  @tiffanylue i know  i was listenin to bad habi...       empty\n",
       "1  Layin n bed with a headache  ughhhh...waitin o...     sadness\n",
       "2                Funeral ceremony...gloomy friday...     sadness\n",
       "3               wants to hang out with friends SOON!  enthusiasm\n",
       "4  @dannycastillo We want to trade with someone w...     neutral"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keeping only the neccessary columns\n",
    "data = data[['content','sentiment']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove lessed labeled emotion for better prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del_row=['empty','enthusiasm','boredom','anger']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=data[data['sentiment']!='empty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=data[data['sentiment']!='enthusiasm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=data[data['sentiment']!='boredom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=data[data['sentiment']!='anger']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38125, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "38125 data for 9 category prediction is very less encouraging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral      8638\n",
      "worry        8459\n",
      "happiness    5209\n",
      "sadness      5165\n",
      "love         3842\n",
      "surprise     2187\n",
      "fun          1776\n",
      "relief       1526\n",
      "hate         1323\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/preprocessing/text.py:89: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "data['content'] = data['content'].apply(lambda x: x.lower())\n",
    "data['content'] = data['content'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "\n",
    "print(data['sentiment'].value_counts())\n",
    "\n",
    "for idx,row in data.iterrows():\n",
    "    row[0] = row[0].replace('rt',' ')\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS, split=' ')\n",
    "tokenizer.fit_on_texts(data['content'].values)\n",
    "X = tokenizer.texts_to_sequences(data['content'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        layin n bed with a headache  ughhhhwaitin on y...\n",
       "2                            funeral ceremonygloomy friday\n",
       "4        dannycastillo we want to trade with someone wh...\n",
       "5        repinging ghostridah14 why didnt you go to pro...\n",
       "6        i should be sleep but im not thinking about an...\n",
       "7                            hmmm httpwwwdjherocom is down\n",
       "8                    charviray charlene my love i miss you\n",
       "9                   kelcouch im sorry  at least its friday\n",
       "10                                        cant fall asleep\n",
       "11                                 choked on her retainers\n",
       "12       ugh i have to beat this stupid song to get to ...\n",
       "13       brodyjenner if u watch the hills in london u w...\n",
       "14                                            got the news\n",
       "15           the storm is here and the electricity is gone\n",
       "16                                     annarosekerr agreed\n",
       "17       so sleepy again and its not even that late i f...\n",
       "18       perezhilton lady gaga tweeted about not being ...\n",
       "19       how are you convinced that i have always wante...\n",
       "20       raaaaaaek oh too bad i hope it gets better ive...\n",
       "21       wondering why im awake at 7amwriting a new son...\n",
       "22       no topic maps talks at the balisage markup con...\n",
       "23       i ate something i dont know what it is why do ...\n",
       "24       so tired and i think im definitely going to ge...\n",
       "25       on my way home n having 2 deal w underage girl...\n",
       "26       isaacmascote  im sorry people are so rude to y...\n",
       "27       damm servers still down  i need to hit 80 befo...\n",
       "28       fudge just bsd that whole paper so tired ugh i...\n",
       "29             i hate cancer i hate it i hate it i hate it\n",
       "30       it is so annoying when she sta s typing on her...\n",
       "31                                cynthia_123 i cant sleep\n",
       "                               ...                        \n",
       "39968    muffinwomanxo eh u dont like retro tisk tisk w...\n",
       "39969    anyone knows a site like the swedish site quot...\n",
       "39970                                     ib happy bi hday\n",
       "39972    acchanosaurus good luck chan gue kmrn bawa bac...\n",
       "39973    good morningmidday nation  formula one in one ...\n",
       "39974    to my pretty lady nikkiwoods happy mothers day...\n",
       "39976                         right coursework now promise\n",
       "39977                           buddinggenius you dont say\n",
       "39978    givemestrength bloody feds they lost last stat...\n",
       "39979                     prinsezha awesome whadya get her\n",
       "39980    sitting in gatwick going home for a week cant ...\n",
       "39981               maynaseric good luck with your auction\n",
       "39982    hey guys if you have something to ask just ask...\n",
       "39983    astronick not really just leaving flat now on ...\n",
       "39984                           iscreamshinki oh thats why\n",
       "39985    mcmedia husband is golfing amp the toddler and...\n",
       "39986    going to watch boy in the striped pjs hope i d...\n",
       "39987    gave the bikes a thorough wash degrease it and...\n",
       "39988    had such and amazing time last night mcfly wer...\n",
       "39989    his snoring is so annoying n it keeps me from ...\n",
       "39990    shonali i think the lesson of the day is not t...\n",
       "39991    lovelylisaj can you give me the link for the k...\n",
       "39992    jasimmo ooo showing of your french skills lol ...\n",
       "39993    sendsome2me haha yeah twitter has many uses fo...\n",
       "39994                          succesfully following tayla\n",
       "39995                                      johnlloydtaylor\n",
       "39996                       happy mothers day  all my love\n",
       "39997    happy mothers day to all the mommies out there...\n",
       "39998    niariley wassup beautiful follow me  peep out ...\n",
       "39999    mopedronin bullet train from tokyo    the gf a...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The giving dataset contains lots of garbage words (*as obvious because people use slang and shortform at twitter due to 140 chars limitation*) as you see above, many word like layin,ughhhhwaitin,lovelylisaj,sendsome2me, n, niariley, mopedronin etc have no meaning which makes data noisy very noisy which ultimately give poor output**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51148 unique tokens.\n",
      "Shape of data tensor: (38125, 2)\n",
      "Shape of label tensor: (38125, 9)\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "Y = data['sentiment']\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(Y)\n",
    "Y=le.transform(Y) \n",
    "labels = to_categorical(np.asarray(Y))\n",
    "print('Shape of data tensor:', X.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "VALIDATION_SPLIT=0.20\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * X.shape[0])\n",
    "\n",
    "x_train = X[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = X[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1917495 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.42B.300d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "1ba3cf60-a83c-9c21-05e0-b14303027e93",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py:86: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, dropout=0.25, recurrent_dropout=0.2)`\n",
      "  '` call to the Keras 2 API: ' + signature)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 30, 300)           15344700  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 196)               389648    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 1773      \n",
      "=================================================================\n",
      "Total params: 15,736,121.0\n",
      "Trainable params: 391,421.0\n",
      "Non-trainable params: 15,344,700.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 300\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.25))\n",
    "model.add(Dense(9,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "15f4ee61-47e4-88c4-4b81-98a85237333f"
   },
   "source": [
    "Hereby I declare the train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_cell_guid": "b35748b8-2353-3db2-e571-5fd22bb93eb0",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25543, 32) (25543, 9)\n",
      "(12582, 32) (12582, 9)\n"
     ]
    }
   ],
   "source": [
    "# Y = pd.get_dummies(data['sentiment']).values\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "# print(X_train.shape,Y_train.shape)\n",
    "# print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2a775979-a930-e627-2963-18557d7bf6e6"
   },
   "source": [
    "Here we train the Network. We should run much more than 7 epoch, but I would have to wait forever for kaggle, so it is 7 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30500 samples, validate on 7625 samples\n",
      "Epoch 1/10\n",
      "30500/30500 [==============================] - 50s - loss: 1.8402 - acc: 0.3323 - val_loss: 1.7368 - val_acc: 0.3656\n",
      "Epoch 2/10\n",
      "30500/30500 [==============================] - 49s - loss: 1.7260 - acc: 0.3745 - val_loss: 1.7034 - val_acc: 0.3794\n",
      "Epoch 3/10\n",
      "30500/30500 [==============================] - 49s - loss: 1.6785 - acc: 0.3912 - val_loss: 1.6928 - val_acc: 0.3819\n",
      "Epoch 4/10\n",
      "30500/30500 [==============================] - 49s - loss: 1.6503 - acc: 0.4017 - val_loss: 1.6828 - val_acc: 0.3870\n",
      "Epoch 5/10\n",
      "30500/30500 [==============================] - 49s - loss: 1.6207 - acc: 0.4116 - val_loss: 1.6899 - val_acc: 0.3847\n",
      "Epoch 6/10\n",
      "30500/30500 [==============================] - 49s - loss: 1.5922 - acc: 0.4197 - val_loss: 1.7012 - val_acc: 0.3761\n",
      "Epoch 7/10\n",
      "30500/30500 [==============================] - 49s - loss: 1.5626 - acc: 0.4304 - val_loss: 1.6891 - val_acc: 0.3878\n",
      "Epoch 8/10\n",
      "30500/30500 [==============================] - 49s - loss: 1.5280 - acc: 0.4455 - val_loss: 1.6964 - val_acc: 0.3847\n",
      "Epoch 9/10\n",
      "30500/30500 [==============================] - 49s - loss: 1.4901 - acc: 0.4622 - val_loss: 1.7125 - val_acc: 0.3836\n",
      "Epoch 10/10\n",
      "30500/30500 [==============================] - 49s - loss: 1.4488 - acc: 0.4778 - val_loss: 1.7440 - val_acc: 0.3780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aca3a67a58>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4ebd7bc1-53c0-0e31-a0b0-b6d0a3017434"
   },
   "source": [
    "Extracting a validation set, and measuring score and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "_cell_guid": "a970f412-722f-6d6d-72c8-325d0901ccef",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 2.38\n",
      "acc: 0.33\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "018ebf39-9414-27d0-232c-a34de051feaf"
   },
   "source": [
    "Finally measuring the number of correct guesses.  It is clear that finding negative tweets goes very well for the Network but deciding whether is positive is not really. My educated guess here is that the positive training set is dramatically smaller than the negative, hence the \"bad\" results for positive tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "_cell_guid": "24c64f46-edd1-8d0b-7c7c-ef50fd26b2fd",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.27604849  0.27941658  0.27901639  0.28271564  0.28608924]\n",
      "0.283016393443\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(inp, target)\n",
    "predictions = gbm.predict(X_test)\n",
    "from sklearn import cross_validation\n",
    "acc_scores = cross_validation.cross_val_score(gbm,inp, target, cv=5)\n",
    "print (acc_scores)\n",
    "print(metrics.accuracy_score(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.258326776816\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "logisticReg = linear_model.LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "logisticReg.fit(inp, target)\n",
    "predictions = logisticReg.predict(X_test)\n",
    "print(metrics.accuracy_score(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 ..., 0 0 0]\n",
      " [0 0 0 ..., 1 0 0]\n",
      " [0 0 0 ..., 1 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data['sentiment'])\n",
    "print(Y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = data['sentiment']\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(Y)\n",
    "Y=le.transform(Y) \n",
    "inp, X_test, target, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 4, ..., 3, 1, 3])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 185,
  "_is_fork": false,
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
